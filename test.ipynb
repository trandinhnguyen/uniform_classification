{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_2_float(line):\n",
    "    return [float(e) for e in line.strip().split()]\n",
    "\n",
    "\n",
    "def read_txt(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    lines = [str_2_float(line) for line in lines]\n",
    "    return np.array(lines)\n",
    "\n",
    "\n",
    "def crop_imgs(root):\n",
    "    os.makedirs(os.path.join(root, \"no\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(root, \"yes\"), exist_ok=True)\n",
    "\n",
    "    for img_name in os.listdir(os.path.join(root, \"images\")):\n",
    "        name = os.path.splitext(img_name)[0]\n",
    "        metadata_name = name + \".txt\"\n",
    "        img = np.array(Image.open(os.path.join(root, \"images\", img_name)))\n",
    "        metadatas = read_txt(os.path.join(root, \"labels\", metadata_name))\n",
    "        height, width = img.shape[:2]\n",
    "\n",
    "        for i in range(metadatas.shape[0]):\n",
    "            class_id, x_center, y_center, w, h = metadatas[i]\n",
    "            start_x = int(width * (x_center - w / 2))\n",
    "            start_y = int(height * (y_center - h / 2))\n",
    "            end_x = start_x + int(w * width)\n",
    "            end_y = start_y + int(h * height)\n",
    "            img_instance = img[start_y:end_y, start_x:end_x, :]\n",
    "\n",
    "            if class_id == 0:  # uniform\n",
    "                Image.fromarray(img_instance).save(\n",
    "                    os.path.join(root, \"yes\", f\"{name}_{i}.jpg\")\n",
    "                )\n",
    "            else:  # no uniform\n",
    "                Image.fromarray(img_instance).save(\n",
    "                    os.path.join(root, \"no\", f\"{name}_{i}.jpg\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"scraped/train\"\n",
    "for dir in os.listdir(root):\n",
    "    os.makedirs(\"processed_imgs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViT(\n",
       "  (vit): ViTForImageClassification(\n",
       "    (vit): ViTModel(\n",
       "      (embeddings): ViTEmbeddings(\n",
       "        (patch_embeddings): ViTPatchEmbeddings(\n",
       "          (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): ViTEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x ViTLayer(\n",
       "            (attention): ViTSdpaAttention(\n",
       "              (attention): ViTSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): ViTSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ViTIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): ViTOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (classifier): Sequential(\n",
       "      (0): Dropout(p=0.4, inplace=False)\n",
       "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import lightning as L\n",
    "import torchmetrics\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import ViTForImageClassification, ViTConfig\n",
    "from models import *\n",
    "\n",
    "model = ViT.load_from_checkpoint(\n",
    "    \"uniform_classification/easop82o/checkpoints/best.ckpt\"\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19158172607421875\n",
      "0.005522251129150391\n",
      "0.004069089889526367\n",
      "0.003927946090698242\n",
      "0.0039038658142089844\n",
      "0.003877401351928711\n",
      "0.003873109817504883\n",
      "0.0038738250732421875\n",
      "0.004007816314697266\n",
      "0.0038759708404541016\n",
      "0.00384521484375\n",
      "0.003873109817504883\n",
      "0.0038454532623291016\n",
      "0.0041201114654541016\n",
      "0.0038690567016601562\n",
      "0.003873109817504883\n",
      "0.003906726837158203\n",
      "0.0039293766021728516\n",
      "0.0038907527923583984\n",
      "0.005156517028808594\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for _ in range(20):\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        output = model(torch.randn(1, 3, 224, 224, device=\"cuda\"))\n",
    "    print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4584, 9168, 14878)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "long = glob.glob(\"datasets/uniform_bidv/raw/yes/bidv_long_front/*\")\n",
    "short = glob.glob(\"datasets/uniform_bidv/raw/yes/bidv_short*/*\")\n",
    "no = glob.glob(\"datasets/uniform_bidv/raw/no/*/*\")\n",
    "len(long), len(short), len(no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1837"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(\"datasets/uniform_bidv/raw/no/cropped_inria_person\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = long + short + no\n",
    "y_binary = [1] * (len(long) + len(short)) + [0] * len(no)\n",
    "y_3_classes = [1] * len(long) + [2] * len(short) + [0] * len(no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.1875, random_state=42, stratify=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2988 5951 1.9916331994645247\n",
      "711 1352 1.9015471167369902\n",
      "885 1865 2.1073446327683616\n"
     ]
    }
   ],
   "source": [
    "def get_n_long_short(X, y):\n",
    "    n_long, n_short = 0, 0\n",
    "    for path, label in zip(X, y):\n",
    "        if label == 1:\n",
    "            if \"long\" in path.split(\"/\")[4]:\n",
    "                n_long += 1\n",
    "            else:\n",
    "                n_short += 1\n",
    "    print(n_long, n_short, n_short / n_long)\n",
    "\n",
    "\n",
    "get_n_long_short(X_train, y_train)\n",
    "get_n_long_short(X_val, y_val)\n",
    "get_n_long_short(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.DataFrame({\"path\": X_train, \"label\": y_train})\n",
    "df_val = pd.DataFrame({\"path\": X_val, \"label\": y_val})\n",
    "df_test = pd.DataFrame({\"path\": X_test, \"label\": y_test})\n",
    "\n",
    "df_train.to_csv(\"datasets/uniform_bidv/binary/train.csv\", index=False)\n",
    "df_val.to_csv(\"datasets/uniform_bidv/binary/val.csv\", index=False)\n",
    "df_test.to_csv(\"datasets/uniform_bidv/binary/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_3_classes, test_size=0.2, random_state=42, stratify=y_3_classes\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.1875, random_state=42, stratify=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.DataFrame({\"path\": X_train, \"label\": y_train})\n",
    "df_val = pd.DataFrame({\"path\": X_val, \"label\": y_val})\n",
    "df_test = pd.DataFrame({\"path\": X_test, \"label\": y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    0.519695\n",
       "2    0.320221\n",
       "1    0.160084\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"label\"].value_counts() / len(df_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    0.519674\n",
       "2    0.320140\n",
       "1    0.160186\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val[\"label\"].value_counts() / len(df_val.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    0.519560\n",
       "2    0.320293\n",
       "1    0.160147\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[\"label\"].value_counts() / len(df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"datasets/uniform_bidv/three_classes/train.csv\", index=False)\n",
    "df_val.to_csv(\"datasets/uniform_bidv/three_classes/val.csv\", index=False)\n",
    "df_test.to_csv(\"datasets/uniform_bidv/three_classes/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/uniform_bidv/raw/no/yellow_shirt/00027-564160557.jpg\n",
      "0.7342995169082126\n",
      "datasets/uniform_bidv/raw/no/black_shirt_man/00592-3097153832.jpg\n",
      "0.6519480519480519\n",
      "datasets/uniform_bidv/raw/no/orange_shirt/00313-37124169692.jpg\n",
      "0.7435897435897436\n",
      "datasets/uniform_bidv/raw/no/orange_shirt/00483-3712417139.jpg\n",
      "0.787109375\n",
      "datasets/uniform_bidv/raw/no/green_shirt/00636-38548885452.jpg\n",
      "0.3953488372093023\n",
      "datasets/uniform_bidv/raw/no/black_hoodie_man/00123-2528966523.jpg\n",
      "0.771875\n",
      "datasets/uniform_bidv/raw/no/white_hoodie_woman/00758-15985693792.jpg\n",
      "0.5147058823529411\n",
      "datasets/uniform_bidv/raw/no/violet_shirt/00346-1992601935.jpg\n",
      "0.26382978723404255\n",
      "datasets/uniform_bidv/raw/no/red_shirt/00019-26594043432.jpg\n",
      "0.7884615384615384\n",
      "datasets/uniform_bidv/raw/no/black_jacket_woman/00035-41412510563.jpg\n",
      "0.5283018867924528\n",
      "datasets/uniform_bidv/raw/no/black_jacket_woman/00035-41412510562.jpg\n",
      "0.5617977528089888\n",
      "388.2516100573445 172.78341420379357\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "s_height, s_width = 0, 0\n",
    "for path in no:\n",
    "    img = cv2.imread(path)\n",
    "    s_height += img.shape[0]\n",
    "    s_width += img.shape[1]\n",
    "    if img.shape[0] / img.shape[1] <= 0.8:\n",
    "        print(path)\n",
    "        print(img.shape[0] / img.shape[1])\n",
    "print(s_height / len(no), s_width / len(no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m2024-11-29 10:10:07.823571433 [E:onnxruntime:Default, provider_bridge_ort.cc:1745 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1426 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "\u001b[m\n",
      "\u001b[0;93m2024-11-29 10:10:07.823592849 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:895 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirementsto ensure all dependencies are met.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "session = ort.InferenceSession(\n",
    "    \"mobilenet_v3_softmax_fp16.onnx\",\n",
    "    providers=[\"CUDAExecutionProvider\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
